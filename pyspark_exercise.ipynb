{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishnubablu112/Pyspark_Exercie/blob/main/pyspark_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark homework assignment\n",
        "\n",
        "## Context\n",
        "\n",
        "The goal of this assignment is to get view on your coding workflow & style.  Your main focus should be creating performant & robust code for data manipulations.  \n",
        "\n",
        "For a homework assignment, we cannot grant you access to our infrastructure (Cloudera data platform on prem: a spark cluster deployment on Yarn).  Since the focus is on development, we provided a template notebook to get up and running very quickly on Google Colab.  \n",
        "\n",
        "You have the freedom to perform this assignment on any spark3+ infrastructure.  If want to use a local or cloud setup, go for it!\n",
        "\n",
        "Some of the tasks are open for interpretation.  This allows us to assess business understanding and relevant field experience.  These tasks are not pass or fail checks.  During the interview we'll ask details about the choice(s) you made.\n",
        "\n",
        "For the assignment, you'll be working with store location data.  You might be familiar with the phrase \"Location, location, location\" from the real-estate context.  The same house can have a different selling price based on the location.  In fast moving consumer goods (FMCG), location is one of the most crucial aspects:\n",
        "\n",
        "* Proximity & accessibility to customers increases convenience\n",
        "* Proximity to competitors increases market pressure\n",
        "* It has impact on the supply chain\n",
        "\n",
        "## Evaluation criteria\n",
        "\n",
        "1. Software engineering\n",
        "   1. Clean code (e.g. using meaningful names)\n",
        "   1. Robust & efficient code\n",
        "   1. Styling (e.g. PEP8, or Google style guide)\n",
        "   1. Documentation(e.g. docstrings)\n",
        "   1. Design (e.g. SOLID principles)\n",
        "1. Workflow\n",
        "   1. How you use Git\n",
        "   1. How you structure your assignment\n",
        "   1. Owning mistakes\n",
        "   1. Rationale for design decisions\n",
        "   1. Making your solution accessible to others\n",
        "1. Business context\n",
        "   1. GDPR\n",
        "   1. Fast moving consumer goods\n",
        "1.(optional: own infra) System engineering\n",
        "   1. What setup did you use?\n",
        "   1. How did you set it up?\n",
        "\n",
        "## Deliverables we expect\n",
        "\n",
        "1. Private GitHub repo\n",
        "   1. Colab allows you to save to GitHub\n",
        "   1. Invite my username to your private repo as contributor\n",
        "1. README.md with relevant content\n",
        "1. Code relevant to the assignment\n"
      ],
      "metadata": {
        "id": "MMCkWQR0NQh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google colab spark setup"
      ],
      "metadata": {
        "id": "ZWyMNpBNON-p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RiQgV1aNOJD"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import environ\n",
        "import findspark"
      ],
      "metadata": {
        "id": "V6xZShWarSWL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting environment variables\n",
        "environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "Cvm3vu7cNSzV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Init spark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "8y6JdEBrO43D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# spark.sql.repl.eagerEval.enabled: Property used to format output tables better\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"cg-pyspark-assignment\")\n",
        "    .master(\"local\")\n",
        "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "    .getOrCreate()\n",
        "  )\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "o5-SFL47PQfJ",
        "outputId": "aa2c3e2b-eb9b-4f87-c538-5fd74a3c9055"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7870f0316f90>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a4b7eedaa862:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>cg-pyspark-assignment</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the assignment data\n",
        "\n",
        "This will call the api and save the results in current working directory as .json files"
      ],
      "metadata": {
        "id": "QWtk5p_punuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/clp-places > clp-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/okay-places > okay-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/spar-places > spar-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/dats-places > dats-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/cogo-colpnts > cogo-colpnts.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-UEk918o3Xt",
        "outputId": "a589a106-54cd-46cb-a9b9-650fb7c13c4e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  224k    0  224k    0     0   100k      0 --:--:--  0:00:02 --:--:--  100k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  144k    0  144k    0     0   103k      0 --:--:--  0:00:01 --:--:--  103k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  169k    0  169k    0     0   109k      0 --:--:--  0:00:01 --:--:--  109k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 88519    0 88519    0     0  75659      0 --:--:--  0:00:01 --:--:-- 75721\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  256k    0  256k    0     0   205k      0 --:--:--  0:00:01 --:--:--  205k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment instructions\n",
        "\n",
        "1. Download the data from api\n",
        "1. Create a logger object that logs to a file \"assignment.log\"\n",
        "   1. You can add whatever logging config you want or need\n",
        "   1. At least on Filehandler based on instructions\n",
        "1. implement get_data_by_brand function\n",
        "   1. Follow instructions in docstring\n",
        "   1. df_clp code line should work\n",
        "1. No more handholding ... :-)\n",
        "1. Create a single object (dataframe) that:\n",
        "   1. Contains data from **all brands**\n",
        "      1. Not every brand has the same columns!\n",
        "   1. Drop placeSearchOpeningHours\n",
        "   1. You can keep sellingPartners as an array\n",
        "   1. Extract \"postal_code\" from address\n",
        "   1. Create new column \"province\" derived from postal_code\n",
        "   1. Transform geoCoordinates into lat and lon column\n",
        "   1. One-hot-encode the handoverServices\n",
        "   1. Pretend houseNumber and streetName are GDPR sensitive.\n",
        "      1. How would you anonymize this data for unauthorized users?\n",
        "      1. (optional) Implement the above\n",
        "      1. How would you show the real data to authorized users?\n",
        "      1. (optional) Implement the above\n",
        "1. Save the end result as a parquet file\n",
        "   1. (optional)partitioning?\n",
        "\n",
        "**postal_code** logic:\n",
        "* \"Brussel\": 1000-1299  \n",
        "* \"Waals-Brabant\": 1300-1499  \n",
        "* \"Vlaams-Brabant\": 1500-1999, 3000-3499  \n",
        "* \"Antwerpen\": 2000-2999  \n",
        "* \"Limburg\": 3500-3999  \n",
        "* \"Luik\": 4000-4999  \n",
        "* \"Namen\": 5000-5999  \n",
        "* \"Henegouwen\": 6000-6599,7000-7999  \n",
        "* \"Luxemburg\": 6600-6999  \n",
        "* \"West-Vlaanderen\": 8000-8999  \n",
        "* \"Oost-Vlaanderen\": 9000-9999"
      ],
      "metadata": {
        "id": "VyvIVnyivCpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import statements should go here\n",
        "from logging import getLogger, Logger"
      ],
      "metadata": {
        "id": "DlsAnfxdxsT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify this based on assignment instructions\n",
        "LOGGER = getLogger()"
      ],
      "metadata": {
        "id": "ShwdnfjSxm2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_by_brand(brand: str, logger: Logger = LOGGER):\n",
        "  \"\"\"Fetch input data based on brand.\n",
        "\n",
        "  Please add a column to the data indicating the input brand\n",
        "  Please add minimum one sanity check for loading the data\n",
        "  Please log things you consider relevant\n",
        "\n",
        "  Args:\n",
        "      brand: allowed values are (clp, okay, spar, dats, cogo)\n",
        "      logger: Logger object for logging\n",
        "\n",
        "  Returns:\n",
        "      The relevant dataframe\n",
        "  \"\"\"\n",
        "  raise NotImplementedError()"
      ],
      "metadata": {
        "id": "_wZhHFGFXkW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_clp code snippet, this should work as expected\n",
        "df_clp = get_data_by_brand(brand=\"clp\", logger=LOGGER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "chz34qgi0vvK",
        "outputId": "4d759a3b-3fb4-4705-9d52-732ce40c906f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7e8537b7ea51>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# df_clp code snippet, this should work as expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_clp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_by_brand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"clp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOGGER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-4b8764f419a4>\u001b[0m in \u001b[0;36mget_data_by_brand\u001b[0;34m(brand, logger)\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mThe\u001b[0m \u001b[0mrelevant\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \"\"\"\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "raCPEOOs8tWU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}